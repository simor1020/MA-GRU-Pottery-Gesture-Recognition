import cv2
import mediapipe as mp
from tensorflow.keras import layers, Model, Sequential
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
from tensorflow.keras import layers, Model, Sequential
import numpy as np
import socket
import os
from pathlib import Path

mp_holistic = mp.solutions.holistic # Holistic model
mp_drawing = mp.solutions.drawing_utils # Drawing utilities

sequence = []
sentence = []
threshold = 0.8
actions = np.array(['1','2','3','4','5','6','7','8','9','10'])

# MediaPipe HandsÂàùÂßãÂåñ
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False,
                       max_num_hands=2,
                       min_detection_confidence=0.5,
                       min_tracking_confidence=0.5)


def mediapipe_detection(image, model):
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB
    image.flags.writeable = False                  # Image is no longer writeable
    results = model.process(image)                 # Make prediction
    image.flags.writeable = True                   # Image is now writeable
    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR
    return image, results

def draw_styled_landmarks(image, results):
    # Draw face connections
    """
    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_CONTOURS,
                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),
                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)
                             )
    """
    # Draw pose connections
    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)
                             )
    # Draw left hand connections
    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)
                             )
    # Draw right hand connections
    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS,
                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),
                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)
                             )

def landmarks_to_numpy(results):
    shape = (2, 21, 3)
    landmarks = results.multi_hand_landmarks
    if landmarks is None:
        return np.zeros(shape)
    elif len(landmarks) == 1:
        label = results.multi_handedness[0].classification[0].label
        hand = landmarks[0]
        if label == "Left":
            return np.array([np.array([[hand.landmark[i].x, hand.landmark[i].y, hand.landmark[i].z] for i in range(21)]),
                             np.zeros((21, 3))])
        else:
            return np.array([np.zeros((21, 3)),
                             np.array([[hand.landmark[i].x, hand.landmark[i].y, hand.landmark[i].z] for i in range(21)])])
    elif len(landmarks) == 2:
        lh_idx, rh_idx = 0, 0
        for idx, hand_type in enumerate(results.multi_handedness):
            label = hand_type.classification[0].label
            if label == 'Left':
                lh_idx = idx
            if label == 'Right':
                rh_idx = idx
        lh = np.array([[landmarks[lh_idx].landmark[i].x, landmarks[lh_idx].landmark[i].y, landmarks[lh_idx].landmark[i].z] for i in range(21)])
        rh = np.array([[landmarks[rh_idx].landmark[i].x, landmarks[rh_idx].landmark[i].y, landmarks[rh_idx].landmark[i].z] for i in range(21)])
        return np.array([lh, rh])
    else:
        return np.zeros((2, 21, 3))

def relative_coordinate(arr, point):
    return arr - point

def standardization(hand_arr):
    if np.all(hand_arr == 0):
        return hand_arr
    mean = np.mean(hand_arr, axis=0)
    std = np.std(hand_arr, axis=0)
    std = np.where(std < 1e-6, 1.0, std)
    return (hand_arr - mean) / std

def process_mark_data(hand_arr):
    lh_root = hand_arr[0, 0]
    rh_root = hand_arr[1, 0]

    lh_marks = relative_coordinate(hand_arr[0, 1:], lh_root)
    rh_marks = relative_coordinate(hand_arr[1, 1:], rh_root)

    lh_marks = standardization(lh_marks)
    rh_marks = standardization(rh_marks)

    Lmark=lh_marks.flatten()
    Rmark=rh_marks.flatten()
    return np.concatenate([Lmark, Rmark])
def extract_keypoints(results):
    #pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)
    #face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)
    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)
    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)
    return np.concatenate([lh, rh])

# 1. ‰ºòÂåñÂêéÁöÑÈÄöÈÅìÊ≥®ÊÑèÂäõ (‰ΩøÁî®Êõ¥È´òÊïàÁöÑÂÖ®ËøûÊé•Â±ÇÂíåÊøÄÊ¥ªÂáΩÊï∞)
class ChannelAttention(layers.Layer):
    def __init__(self, ratio=8, activation='relu', **kwargs):
        super(ChannelAttention, self).__init__(**kwargs)
        self.ratio = ratio
        self.activation = activation

    def build(self, input_shape):
        self.channel = input_shape[-1]
        # ‰ΩøÁî®Êõ¥Â∞ëÁöÑÂèÇÊï∞ÔºöÂÖ±‰∫´ÁöÑÂÖ®ËøûÊé•Â±Ç
        self.shared_dense = layers.Dense(self.channel // self.ratio, activation=self.activation, use_bias=False)
        self.output_dense = layers.Dense(self.channel, activation='sigmoid', use_bias=False)

    def call(self, inputs):
        # ‰ΩøÁî®ÂÖ®Â±ÄÂπ≥ÂùáÊ±†ÂåñÂíåÊúÄÂ§ßÊ±†Âåñ
        avg_pool = tf.reduce_mean(inputs, axis=1)
        max_pool = tf.reduce_max(inputs, axis=1)

        # ÂÖ±‰∫´ÊùÉÈáçÁöÑMLP
        avg_out = self.output_dense(self.shared_dense(avg_pool))
        max_out = self.output_dense(self.shared_dense(max_pool))

        scale = avg_out + max_out
        return inputs * scale[:, None, :]  # ÂπøÊí≠


# 2. ‰ºòÂåñÂêéÁöÑÊó∂Èó¥Ê≥®ÊÑèÂäõ (Êü•ËØ¢ÈîÆÂÖ±‰∫´ÔºåÂáèÂ∞ëÂèÇÊï∞)
class TemporalAttention(layers.Layer):
    def __init__(self, units, **kwargs):
        super(TemporalAttention, self).__init__(**kwargs)
        self.units = units
        # ‰ΩøÁî®ÂÖ±‰∫´ÁöÑÊäïÂΩ±Áü©ÈòµÁî®‰∫ékeyÂíåqueryÔºåÂáèÂ∞ëÂèÇÊï∞
        self.shared_projection = layers.Dense(units, activation='tanh', use_bias=False)
        self.V = layers.Dense(1, use_bias=False)

    def call(self, query, values):
        # ÊäïÂΩ±Êü•ËØ¢ÂíåÈîÆÂà∞Áõ∏ÂêåÁ©∫Èó¥
        projected_query = self.shared_projection(query)  # [batch_size, units]
        projected_values = self.shared_projection(values)  # [batch_size, timesteps, units]

        # Êâ©Â±ïÊü•ËØ¢Áª¥Â∫¶‰ª•ÂåπÈÖçÂÄº
        projected_query = tf.expand_dims(projected_query, 1)  # [batch_size, 1, units]

        # ËÆ°ÁÆóÊ≥®ÊÑèÂäõÂàÜÊï∞
        score = self.V(tf.nn.tanh(projected_values + projected_query))  # [batch_size, timesteps, 1]
        attention_weights = tf.nn.softmax(score, axis=1)  # [batch_size, timesteps, 1]

        # Â∫îÁî®Ê≥®ÊÑèÂäõÊùÉÈáç
        context_vector = tf.reduce_sum(attention_weights * values, axis=1)  # [batch_size, feature_dim]
        return context_vector


# 3. ‰ºòÂåñÂêéÁöÑÊó∂Á©∫Ê≥®ÊÑèÂäõÊ®°Âùó
class SpatioTemporalAttention(layers.Layer):
    def __init__(self, ratio=8, temporal_units=64, **kwargs):
        super(SpatioTemporalAttention, self).__init__(**kwargs)
        self.ratio = ratio
        self.temporal_units = temporal_units
        self.channel_attention = ChannelAttention(ratio)
        self.temporal_attention = TemporalAttention(temporal_units)

    def call(self, inputs):
        # ÈÄöÈÅìÊ≥®ÊÑèÂäõ
        channel_refined = self.channel_attention(inputs)  # [batch_size, timesteps, features]

        # Êó∂Èó¥Ê≥®ÊÑèÂäõ - ‰ΩøÁî®ÊúÄÂêé‰∏Ä‰∏™Êó∂Èó¥Ê≠•‰Ωú‰∏∫Êü•ËØ¢
        temporal_refined = self.temporal_attention(
            query=channel_refined[:, -1, :],  # ÊúÄÂêé‰∏Ä‰∏™Êó∂Èó¥Ê≠•
            values=channel_refined
        )
        return temporal_refined


# 4. ËΩªÈáèÁ∫ßÂèØÂàÜÁ¶ªÂç∑ÁßØÂùó
def lightweight_separable_conv_block(inputs, filters, kernel_size, pool_size=2):
    # Ê∑±Â∫¶ÂèØÂàÜÁ¶ªÂç∑ÁßØÂáèÂ∞ëÂèÇÊï∞
    x = layers.SeparableConv1D(filters, kernel_size, padding='same', use_bias=False)(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.ReLU()(x)
    # ÂèØÈÄâÁöÑ‰∏ãÈááÊ†∑
    if pool_size > 1:
        x = layers.AveragePooling1D(pool_size=pool_size, padding='same')(x)
    return x


# 5. ÊûÑÂª∫‰ºòÂåñÂêéÁöÑÂÆåÊï¥Ê®°Âûã
def build_enhanced_model(input_shape=(150, 120), num_classes=3):
    inputs = layers.Input(shape=input_shape)

    # Èò∂ÊÆµ1ÔºöÂ§öÂ∞∫Â∫¶ÁâπÂæÅÊèêÂèñ (ÂèÇÊï∞ÂáèÂ∞ë)
    # ÂàÜÊîØ1ÔºöÂéüÂßãÂ∞∫Â∫¶ - ‰ΩøÁî®padding='same'‰øùÊåÅÂ∞∫ÂØ∏
    branch1 = layers.SeparableConv1D(64, 5, padding='same', use_bias=False)(inputs)
    branch1 = layers.BatchNormalization()(branch1)
    branch1 = layers.ReLU()(branch1)

    # ÂàÜÊîØ2Ôºö‰∏ãÈááÊ†∑Â∞∫Â∫¶ - Á°Æ‰øù‰∏äÈááÊ†∑ÂêéÂ∞∫ÂØ∏ÂåπÈÖç
    branch2 = layers.AveragePooling1D(2, padding='same')(inputs)  # ‰ΩøÁî®padding='same'[1](@ref)
    branch2 = layers.SeparableConv1D(64, 3, padding='same', use_bias=False)(branch2)
    branch2 = layers.BatchNormalization()(branch2)
    branch2 = layers.ReLU()(branch2)
    branch2 = layers.UpSampling1D(2)(branch2)

    # Ê£ÄÊü•Âπ∂Ë∞ÉÊï¥Â∞∫ÂØ∏ÔºàÂ¶ÇÊûúÈúÄË¶ÅÔºâ
    # Â¶ÇÊûúÂ∞∫ÂØ∏‰ªçÊúâÂæÆÂ∞èÂ∑ÆÂºÇÔºåÂèØ‰ª•‰ΩøÁî®Cropping1DÊàñZeroPadding1D[1](@ref)
    # branch2 = layers.Cropping1D(cropping=(0, 1))(branch2)  # Â¶ÇÊûúÈúÄË¶ÅË£ÅÂâ™
    # ÊàñËÄÖ branch1 = layers.ZeroPadding1D(padding=(0, 1))(branch1)  # Â¶ÇÊûúÈúÄË¶ÅÂ°´ÂÖÖ

    # ÁâπÂæÅËûçÂêà - Áé∞Âú®ÂΩ¢Áä∂Â∫îËØ•ÂåπÈÖç‰∫Ü
    merged = layers.Concatenate()([branch1, branch2])

    # Èò∂ÊÆµ2ÔºöÂèåÂêëGRUÊõø‰ª£LSTM (ÈÄüÂ∫¶Êõ¥Âø´)
    gru_out = layers.Bidirectional(
        layers.GRU(96, return_sequences=True, dropout=0.2)  # ÂáèÂ∞ëÂçïÂÖÉÊï∞
    )(merged)

    # Èò∂ÊÆµ3Ôºö‰ºòÂåñÁöÑÊó∂Á©∫Ê≥®ÊÑèÂäõÊú∫Âà∂
    attention_output = SpatioTemporalAttention()(gru_out)

    # Èò∂ÊÆµ4ÔºöÊÆãÂ∑ÆÁâπÂæÅÂº∫Âåñ + ÂÖ®Â±ÄÁâπÂæÅ
    shortcut = layers.GlobalAvgPool1D()(merged)
    combined = layers.Concatenate()([attention_output, shortcut])

    # Èò∂ÊÆµ5ÔºöÂä®ÊÄÅÂàÜÁ±ªÂ§¥ (ÂáèÂ∞ëÂ±ÇÊï∞)
    x = layers.Dense(128, activation='relu')(combined)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(num_classes, activation='softmax')(x)

    return Model(inputs, outputs)
# ÊîπËøõ3Ôºö‰ºòÂåñËÆ≠ÁªÉÈÖçÁΩÆ


model = build_enhanced_model(input_shape=(150, 120),
                             num_classes=actions.shape[0])

model.load_weights('best.h5')

colors = [(245, 117, 16), (117, 245, 16), (16, 117, 245),(16, 117, 245),(100,200,255),(245, 117, 16),
          (150, 123, 16), (185, 220, 245),(210, 117, 245),(50,100,255)]#Âõõ‰∏™Âä®‰ΩúÁöÑÊ°ÜÊ°ÜÔºåË¶ÅÂ¢ûÂä†Âä®‰ΩúÊï∞ÁõÆÔºåÂ∞±Â§öÂä†RGBÂÖÉÁªÑ


def prob_viz(res, actions, input_frame, colors):
    output_frame = input_frame.copy()
    for num, prob in enumerate(res):
        cv2.rectangle(output_frame, (0, 60 + num * 40), (int(prob * 100), 90 + num * 40), colors[num], -1)
        cv2.putText(output_frame, actions[num], (0, 85 + num * 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2,
                    cv2.LINE_AA)
    return output_frame

# -------------------------------
# 4. ‰∏ªÂáΩÊï∞ÔºöÂ§ÑÁêÜÂõæÁâáÂ∫èÂàó
# -------------------------------
def predict_from_image_folder(folder_path, model, display=True):
    global sequence, sentence

    folder = Path(folder_path)
    if not folder.exists() or not folder.is_dir():
        print(f"‚ùå Êñá‰ª∂Â§π‰∏çÂ≠òÂú®Êàñ‰∏çÊòØÁõÆÂΩï: {folder}")
        return

    # ÊîØÊåÅÁöÑÂõæÂÉèÊ†ºÂºè
    image_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = [f for f in folder.iterdir() if f.suffix.lower() in image_exts]
    image_files.sort(key=lambda x: x.name)  # ÊåâÊñá‰ª∂ÂêçÊéíÂ∫è

    if len(image_files) == 0:
        print("‚ùå Êú™ÊâæÂà∞‰ªª‰ΩïÂõæÁâáÊñá‰ª∂„ÄÇ")
        return

    print(f"üìÅ ÊâæÂà∞ {len(image_files)} Âº†ÂõæÁâáÔºåÂºÄÂßãÂ§ÑÁêÜ...")

    # Â¶ÇÊûú‰∏çÊòæÁ§∫ÔºåÂàõÂª∫‰∏Ä‰∏™Á™óÂè£Áî®‰∫éÊåâ‰ªªÊÑèÈîÆÈÄÄÂá∫
    if display:
        cv2.namedWindow('Gesture Recognition', cv2.WINDOW_AUTOSIZE)

    for img_file in image_files:
        frame = cv2.imread(str(img_file))
        if frame is None:
            print(f"‚ö†Ô∏è Êó†Ê≥ïËØªÂèñÂõæÁâá: {img_file}")
            continue

        # ËΩ¨‰∏∫ RGB ËøõË°å MediaPipe Â§ÑÁêÜ
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = hands.process(image_rgb)

        # ÊèêÂèñÂπ∂Â§ÑÁêÜÂÖ≥ÈîÆÁÇπ
        hand_array = landmarks_to_numpy(results)
        keypoints = process_mark_data(hand_array)
        sequence.append(keypoints)
        sequence = sequence[-150:]  # ‰øùÁïôÊúÄËøë30Â∏ß

        # Êé®ÁêÜ
        if len(sequence) == 150:
            res = model.predict(np.expand_dims(sequence, axis=0))[0]
            pred_class = actions[np.argmax(res)]
            pred_prob = res[np.argmax(res)]

            # Êõ¥Êñ∞Âè•Â≠ê
            if pred_prob > threshold:
                if len(sentence) == 0 or pred_class != sentence[-1]:
                    sentence.append(pred_class)
                    print(f"üéØ ËØÜÂà´Âä®‰Ωú: {pred_class} (ÁΩÆ‰ø°Â∫¶: {pred_prob:.2f})")

            # Âè™‰øùÁïôÊúÄËøë5‰∏™Âä®‰Ωú
            if len(sentence) > 5:
                sentence = sentence[-5:]

            # ÂèØËßÜÂåñÊ¶ÇÁéá
            frame = prob_viz(res, actions, frame, colors)

        # ÊòæÁ§∫ÂΩìÂâçÂ∏ßÂíåÂ∑≤ËØÜÂà´Âä®‰Ωú
        if display:
            cv2.rectangle(frame, (0, 0), (640, 40), (245, 117, 16), -1)
            cv2.putText(frame, ' '.join(sentence), (3, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)
            cv2.imshow('Gesture Recognition', frame)

            key = cv2.waitKey(100)  # ÊØèÂ∏ßÂÅú100msÔºåÂèØË∞ÉÂø´/ÊÖ¢
            if key & 0xFF == ord('q'):
                break
        else:
            # ‰∏çÊòæÁ§∫Êó∂Âø´ÈÄüÂ§ÑÁêÜ
            pass

    # ÁªìÊùüÂêéËæìÂá∫ÊúÄÁªàÁªìÊûú
    print("\n" + "="*50)
    print("‚úÖ Êé®ÁêÜÂÆåÊàêÔºÅ")
    print(f"ËØÜÂà´Âá∫ÁöÑÂä®‰ΩúÂ∫èÂàó: {' -> '.join(sentence)}")
    print("="*50)

    if display:
        cv2.waitKey(0)
        cv2.destroyAllWindows()


# ==============================
# üîß ‰ΩøÁî®ÂâçËØ∑ËÆæÁΩÆ‰ª•‰∏ãÂèòÈáè
# ==============================
if __name__ == "__main__":
    # üìÅ ÂõæÁâáÂ∫èÂàóÊñá‰ª∂Â§πË∑ØÂæÑ
    IMAGE_FOLDER_PATH = r"D:\part\GNM\05B\18"  # <-- ‰øÆÊîπ‰∏∫‰Ω†ÁöÑÂõæÁâáÊñá‰ª∂Â§πË∑ØÂæÑ

   
    if model is None:
        print("‚ùå ÈîôËØØÔºöËØ∑ÂÖàÂä†ËΩΩÊ®°ÂûãÔºÅÂèñÊ∂àÊâßË°å„ÄÇ")
    else:
        predict_from_image_folder(IMAGE_FOLDER_PATH, model, display=True)